{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgb/xzWX1yQl9MF0t4IZkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityashah841/Techgium-Research-GNN-NER/blob/main/Try_teseract_ner_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u_hdSyLVWejn",
        "outputId": "b1810bef-e0ab-41ea-c17a-33b62f7dc684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 7.4 MB/s \n",
            "\u001b[?25hCollecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 67.9 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: xlrd~=1.2.0 in /usr/local/lib/python3.8/dist-packages (from textract) (1.2.0)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.8/dist-packages (from textract) (3.0.4)\n",
            "Collecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Collecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 1.8 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Collecting soupsieve>=1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Collecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "Collecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 72.2 MB/s \n",
            "\u001b[?25hCollecting tzlocal>=2.1\n",
            "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
            "Collecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from python-pptx~=0.6.18->textract) (4.9.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.8/dist-packages (from python-pptx~=0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 67.8 MB/s \n",
            "\u001b[?25hCollecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tzdata\n",
            "  Downloading tzdata-2022.7-py2.py3-none-any.whl (340 kB)\n",
            "\u001b[K     |████████████████████████████████| 340 kB 69.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile, python-pptx\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=09ffb563fbedf7b3b4b15e11c5954a8f7cc597ee64f759884b86826999ea9ca5\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/f0/2c/81637d42670985178b77df6d41b9b6c6dc18c94818447414b9\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6200 sha256=67fab192467f0118e0ddd9b26161122308937d78194d5632e74cc93fffeed210\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/f5/c4/81acab65ab073b5a3e67fd82e4b9accf3dbcf1de39c7b246ec\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35430 sha256=238b327e0e0009a2664287de3ce5a9c18ec176658987f60a9a92d43e09f54178\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/d8/16/1e2d32ad7455728b8af9efdb9d2a0c3d03cd8f2e4be0191b8c\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470952 sha256=1608842087516ffad116fd2000b31e33c51967f15e523987fd22da78352a7051\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/38/58/8530ed1681bfee42349acf166867cc9fb369517b2fce83e599\n",
            "Successfully built docx2txt compressed-rtf olefile python-pptx\n",
            "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, XlsxWriter, tzlocal, soupsieve, pycryptodome, olefile, imapclient, ebcdic, compressed-rtf, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.3 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.16.0 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 soupsieve-2.3.2.post1 textract-1.6.5 tzdata-2022.7 tzlocal-4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECjD7aSjYR08",
        "outputId": "63ac3fc3-9b9b-4a0b-8b2d-b82819d08b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text = textract.process(\"/content/My Resume.pdf\")"
      ],
      "metadata": {
        "id": "KzT7hS4ZWh2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO_JpffxW1Ex",
        "outputId": "0b1ca0c0-d750-487d-977d-af3ef6886137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'A D I T Y A\\n\\nS H A H\\n\\nC O M P U T E R   E N G I N E E R\\n\\nG i t h u b   I D -   a d i t y a s h a h 8 4 1\\n\\n+ 9 1   9 8 2 0 0 1 1 8 1 7\\n\\na d i t y a s h a h 8 4 1 @ g m a i l . c o m\\n\\nP R O F I L E   S U M M A R Y\\n\\nSelf-motivated  level  2  programmer  at  CodeChef,  Code  for  Good  finalist,  skilled  in  Python,  machine\\nlearning, data analysis, loves to indulge in intense projects and competitive programming.\\n\\nP R O F E S S I O N A L   S K I L L S\\n\\nProblem solving\\nTeamwork\\n\\nDetail Oriented\\nLeadership\\n\\nE D U C A T I O N\\n10th ICSE\\nJamnabai Narsee School, Mumbai\\n\\n12th HSC\\nPace Junior Science College, Mumbai\\n\\nComputer Engineering\\nDJ Sanghvi College of Engineering, Mumbai\\n\\nT E C H N I C A L   S K I L L S\\nMachine Learning\\nCV\\nOperating Systems\\nNetworking\\n\\nNLP\\nData Structures\\nAlgorithms\\n\\n2018\\n\\n2020\\n\\n2020 - 2024\\n\\nP O S I T I O N S   O F   R E S P O N S I B I L I T Y\\nMachine Learning Mentee\\nDJS Synapse, Mumbai\\n\\nSeptember, 2021 - August, 2022\\n\\nText  Brew:  Automated  Model  Selection  and  Hyperparameter  Optimization  for  Text\\nClassification  -  The  system  offers  an  automated  way  for  selecting  the  best  transformer\\nmodels and then training these models on the dataset with the right hyperparameter settings.\\nIt is designed in such a manner that it can handle both, binary as well as multiclass prediction\\ndatasets without explicitly stating so.\\n\\nAugust, 2022 - Present\\n\\nConducted a live machine learning workshop at DJ Sanghvi College of Engineering.\\n\\nMachine Learning Head\\nDJS Synapse, Mumbai\\n\\nA C H I E V E M E N T S\\n\\nTSEC Hacks Finalist\\n\\nCode for Good Finalist\\n\\nLevel 2 coder at CodeChef\\n\\n\\x0cP E R S O N A L   P R O J E C T S\\n\\nMultilingual  Text  Summarizer  -  The  program  identifies  the  language  of  the  input  text,\\nsummarizes it and converts the summary into the language as desired by the user.\\nStock  price  predictor  -  The  program  predicts  the  price  of  the  input  stock  over  the  next  30\\ndays  with  immense  accuracy.  Made  by  selecting  LSTM  model  out  of  an  ensemble  of  LSTM\\nmodels, Arima models and FBProphet model.\\nLive  body  angles  calculator  -  The  program  calculates  and  returns  the  angles  made  at  joints\\nover live video footage.\\nLive  Lunges  counter  -  Program  built  to  count  only  correct  repetitions  of  lunges  of  each  leg\\nalternatively.\\nVirtual pong - Pong game built from scratch with the moving mechanism integrated with the\\nmovement of the hands.\\nSpace  Invaders  -  Built  a  fully  functional  space  invaders  game  in  python  using  the  pygame\\nlibrary for the game graphics.\\nCode-Decode - A C-language program which encodes messages using matrix multiplication\\nwhich can only be decoded with the same matrix key.\\nGame  Mania  -  A  collection  of  three  games  made  in  Java  programming  language  accessible\\nfrom the same page.\\n\\nC E R T I F I C A T I O N S   A N D   L I C E N C E S\\nNeural Networks and Deep Learning (Coursera)\\nIssued - July 2022\\nExploratory  Data  Analysis  for  Machine  Learning\\n(Coursera)\\nIssued - February 2022\\nIntroduction to Artificial Intelligence (AI)\\n(Coursera)\\nIssued - January 2022\\nComputer Vision Basics (Coursera)\\nIssued - January 2022\\nWhat \\nintelligence?\\n(Coursera)\\nIssued - January 2022\\n\\nis  \\xe2\\x80\\x9cthe  mind\\xe2\\x80\\x9d  and  what \\n\\nis  artificial\\n\\nIntroduction to Tensorflow (Coursera)\\nIssued - February 2022\\nIBuilding AI Powered Chatbots Without\\nProgramming (Coursera)\\nIssued - January 2022\\nGetting Started with AI using IBM Watson\\n(Coursera)\\nIssued - February 2022\\nPython for Data Science, AI & Development\\n(Coursera)\\nIssued - January 2022\\nMachine Learning Foundations for Product\\nManagers\\n(Coursera)\\nIssued - October 2021\\n\\n\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "mwVvKBX0gkTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = re.sub('[^a-zA-Z]', ' ', text.decode())"
      ],
      "metadata": {
        "id": "iqLLbSDUgmAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "NBM90WpsduhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdhtSr7seXCS",
        "outputId": "7cb8251a-84b3-484c-a7cc-10d5605bd8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2022-12-27 12:37:59.146122: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 13 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-lg==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "2TUcUFvneA6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "hirjDq5IezMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "GyK77B9olFZr",
        "outputId": "b221edda-db9e-4ec4-8562-7b0e71f177a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A D I T Y A  S H A H  C O M P U T E R   E N G I N E E R  G i t h u b   I D     a d i t y a s h a h                                     a d i t y a s h a h         g m a i l   c o m  P R O F I L E   S U M M A R Y  Self motivated  level     programmer  at  CodeChef   Code  for  Good  finalist   skilled  in  Python   machine learning  data analysis  loves to indulge in intense projects and competitive programming   P R O F E S S I O N A L   S K I L L S  Problem solving Teamwork  Detail Oriented Leadership  E D U C A T I O N   th ICSE Jamnabai Narsee School  Mumbai    th HSC Pace Junior Science College  Mumbai  Computer Engineering DJ Sanghvi College of Engineering  Mumbai  T E C H N I C A L   S K I L L S Machine Learning CV Operating Systems Networking  NLP Data Structures Algorithms                           P O S I T I O N S   O F   R E S P O N S I B I L I T Y Machine Learning Mentee DJS Synapse  Mumbai  September         August        Text  Brew   Automated  Model  Selection  and  Hyperparameter  Optimization  for  Text Classification     The  system  offers  an  automated  way  for  selecting  the  best  transformer models and then training these models on the dataset with the right hyperparameter settings  It is designed in such a manner that it can handle both  binary as well as multiclass prediction datasets without explicitly stating so   August         Present  Conducted a live machine learning workshop at DJ Sanghvi College of Engineering   Machine Learning Head DJS Synapse  Mumbai  A C H I E V E M E N T S  TSEC Hacks Finalist  Code for Good Finalist  Level   coder at CodeChef   P E R S O N A L   P R O J E C T S  Multilingual  Text  Summarizer     The  program  identifies  the  language  of  the  input  text  summarizes it and converts the summary into the language as desired by the user  Stock  price  predictor     The  program  predicts  the  price  of  the  input  stock  over  the  next     days  with  immense  accuracy   Made  by  selecting  LSTM  model  out  of  an  ensemble  of  LSTM models  Arima models and FBProphet model  Live  body  angles  calculator     The  program  calculates  and  returns  the  angles  made  at  joints over live video footage  Live  Lunges  counter     Program  built  to  count  only  correct  repetitions  of  lunges  of  each  leg alternatively  Virtual pong   Pong game built from scratch with the moving mechanism integrated with the movement of the hands  Space  Invaders     Built  a  fully  functional  space  invaders  game  in  python  using  the  pygame library for the game graphics  Code Decode   A C language program which encodes messages using matrix multiplication which can only be decoded with the same matrix key  Game  Mania     A  collection  of  three  games  made  in  Java  programming  language  accessible from the same page   C E R T I F I C A T I O N S   A N D   L I C E N C E S Neural Networks and Deep Learning  Coursera  Issued   July      Exploratory  Data  Analysis  for  Machine  Learning  Coursera  Issued   February      Introduction to Artificial Intelligence  AI   Coursera  Issued   January      Computer Vision Basics  Coursera  Issued   January      What  intelligence   Coursera  Issued   January       is   the  mind   and  what   is  artificial  Introduction to Tensorflow  Coursera  Issued   February      IBuilding AI Powered Chatbots Without Programming  Coursera  Issued   January      Getting Started with AI using IBM Watson  Coursera  Issued   February      Python for Data Science  AI   Development  Coursera  Issued   January      Machine Learning Foundations for Product Managers  Coursera  Issued   October        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "print(ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skeijwHQfEwR",
        "outputId": "0a36c63c-7c4f-4160-ffe9-cab13b8edae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('S H A', 13, 18, 'ORG'), ('Y  Self', 210, 217, 'PERSON'), ('Jamnabai Narsee School', 537, 559, 'ORG'), ('Mumbai', 561, 567, 'GPE'), ('Mumbai', 607, 613, 'GPE'), ('Computer Engineering', 615, 635, 'ORG'), ('Mumbai', 671, 677, 'GPE'), ('Mumbai', 909, 915, 'GPE'), ('September         August        Text  Brew   Automated', 917, 971, 'DATE'), ('Model  Selection', 973, 989, 'PRODUCT'), ('Hyperparameter  Optimization', 996, 1024, 'PERSON'), ('August', 1366, 1372, 'DATE'), ('DJ Sanghvi College of Engineering   Machine', 1436, 1479, 'ORG'), ('DJS Synapse', 1494, 1505, 'PERSON'), ('Mumbai', 1507, 1513, 'GPE'), ('Summarizer', 1668, 1678, 'PERSON'), ('next     days', 1925, 1938, 'DATE'), ('LSTM', 1987, 1991, 'ORG'), ('LSTM', 2027, 2031, 'ORG'), ('Arima', 2040, 2045, 'ORG'), ('FBProphet', 2057, 2066, 'ORG'), ('Pong', 2341, 2345, 'NORP'), ('Space', 2439, 2444, 'ORG'), ('Game  Mania     ', 2711, 2727, 'ORG'), ('three', 2746, 2751, 'CARDINAL'), ('Java', 2770, 2774, 'GPE'), ('Deep Learning  Coursera', 2905, 2928, 'ORG'), ('July      Exploratory  Data  Analysis', 2939, 2976, 'ORG'), ('Machine  Learning  Coursera', 2983, 3010, 'ORG'), ('February      Introduction to Artificial Intelligence  AI   Coursera', 3021, 3089, 'WORK_OF_ART'), ('January      Computer', 3100, 3121, 'ORG'), ('January', 3156, 3163, 'DATE'), ('January       ', 3209, 3223, 'DATE'), ('Introduction to Tensorflow  Coursera', 3268, 3304, 'PRODUCT'), ('February      IBuilding AI Powered Chatbots Without Programming  Coursera', 3315, 3388, 'LAW'), ('January      Getting', 3399, 3419, 'LAW'), ('IBM', 3442, 3445, 'ORG'), ('Watson  Coursera', 3446, 3462, 'PRODUCT'), ('February      Python', 3473, 3493, 'WORK_OF_ART'), ('January      Machine Learning Foundations for Product Managers', 3549, 3611, 'ORG'), ('October', 3632, 3639, 'DATE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNh1C51afLlE",
        "outputId": "6f2351c7-5608-4670-b31e-b7c3e83b60d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(S H A,\n",
              " Y  Self,\n",
              " Jamnabai Narsee School,\n",
              " Mumbai,\n",
              " Mumbai,\n",
              " Computer Engineering,\n",
              " Mumbai,\n",
              " Mumbai,\n",
              " September         August        Text  Brew   Automated,\n",
              " Model  Selection,\n",
              " Hyperparameter  Optimization,\n",
              " August,\n",
              " DJ Sanghvi College of Engineering   Machine,\n",
              " DJS Synapse,\n",
              " Mumbai,\n",
              " Summarizer,\n",
              " next     days,\n",
              " LSTM,\n",
              " LSTM,\n",
              " Arima,\n",
              " FBProphet,\n",
              " Pong,\n",
              " Space,\n",
              " Game  Mania     ,\n",
              " three,\n",
              " Java,\n",
              " Deep Learning  Coursera,\n",
              " July      Exploratory  Data  Analysis,\n",
              " Machine  Learning  Coursera,\n",
              " February      Introduction to Artificial Intelligence  AI   Coursera,\n",
              " January      Computer,\n",
              " January,\n",
              " January       ,\n",
              " Introduction to Tensorflow  Coursera,\n",
              " February      IBuilding AI Powered Chatbots Without Programming  Coursera,\n",
              " January      Getting,\n",
              " IBM,\n",
              " Watson  Coursera,\n",
              " February      Python,\n",
              " January      Machine Learning Foundations for Product Managers,\n",
              " October)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequence matching**\n",
        "for similar entities"
      ],
      "metadata": {
        "id": "5c3GJiSkiNBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "HbJe8U26iWbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Load GPT model\n",
        "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Define list of labels\n",
        "labels = ['subject', 'object', 'verb']\n",
        "\n",
        "# Define function to annotate text with GPT\n",
        "def annotate_text_with_gpt(text, labels, model):\n",
        "  # Preprocess input text\n",
        "  input_text = 'Text to classify: ' + text\n",
        "  input_ids = transformers.GPT2Tokenizer.from_pretrained('gpt2').encode(input_text, return_tensors='pt')\n",
        "  \n",
        "  # Generate text with GPT\n",
        "  output = model.generate(input_ids, max_length=len(input_text) + 50, top_p=1.0, top_k=0, eos_token_id=50256)\n",
        "  \n",
        "  # Convert output to string and split on newline character\n",
        "  output_text = transformers.GPT2Tokenizer.from_pretrained('gpt2').decode(output[0], skip_special_tokens=True)\n",
        "  output_text = output_text.strip().split('\\n')\n",
        "  \n",
        "  # Find label in output text\n",
        "  for line in output_text:\n",
        "    for label in labels:\n",
        "      if label in line:\n",
        "        return label\n",
        "        \n",
        "  return 'Label not found'\n",
        "\n",
        "# Annotate text with GPT\n",
        "text = 'This code first loads a pre-trained GPT model and defines a list of labels.'\n",
        "label = annotate_text_with_gpt(text, labels, model)\n",
        "print(f'Text: {text}')\n",
        "print(f'Predicted label: {label}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6KaFzkD5tND",
        "outputId": "ca5e480d-efca-42d4-e9a6-6b4520adb41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This code first loads a pre-trained GPT model and defines a list of labels.\n",
            "Predicted label: Label not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Load GPT model\n",
        "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Define function to annotate text with GPT\n",
        "def annotate_text_with_gpt(text, model):\n",
        "  # Preprocess input text\n",
        "  input_text = 'Text to annotate: ' + text\n",
        "  input_ids = transformers.GPT2Tokenizer.from_pretrained('gpt2').encode(input_text, return_tensors='pt')\n",
        "  \n",
        "  # Generate text with GPT\n",
        "  output = model.generate(input_ids, max_length=len(input_text) + 50, top_p=1.0, top_k=0, eos_token_id=50256)\n",
        "  \n",
        "  # Convert output to string and split on newline character\n",
        "  output_text = transformers.GPT2Tokenizer.from_pretrained('gpt2').decode(output[0], skip_special_tokens=True)\n",
        "  output_text = output_text.strip().split('\\n')\n",
        "  \n",
        "  # Return first line of output text as annotation\n",
        "  return output_text\n",
        "\n",
        "# Annotate text with GPT\n",
        "text = 'This is some example text to annotate.'\n",
        "annotation = annotate_text_with_gpt(text, model)\n",
        "print(f'Text: {text}')\n",
        "print(f'Annotation: {annotation}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEZi6hlc5u4D",
        "outputId": "dd6b61c1-a8c9-4f04-f5dd-fbf7db327a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This is some example text to annotate.\n",
            "Annotation: ['Text to annotate: This is some example text to annotate.', '', 'This is some example text to annotate. Text to annotate: This is some example text to annotate.', '', 'This is some example text to annotate.', '', 'This is some example text to annotate.', '', 'This is some example text to annotate.', '', 'This is some example text to annotate.', '', 'This is some example text to annotate.', '', 'This is some example text to annotate.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9MZe3Stm8LRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}